{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of this assignment will be **automatically graded**. Please take note of the following:\n",
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "- You can add additional cells, but it is not recommended to (re)move cells. Cells required for autograding cannot be moved and cells containing tests cannot be edited.\n",
    "- You are allowed to use a service such as [Google Colaboratory](https://colab.research.google.com/) to work together. However, you **cannot** hand in the notebook that was hosted on Google Colaboratory, but you need to copy your answers into the original notebook and verify that it runs succesfully offline. This is because Google Colaboratory destroys the metadata required for grading.\n",
    "- Name your notebook **exactly** `{TA_name}_{student1_id}_{student2_id}_lab{i}.ipynb`, for example `wouter_12345_67890_lab1.ipynb` (or elise or stephan, depending on your TA), **otherwise your submission will be skipped by our regex and you will get 0 points** (but no penalty as we cannot parse your student ids ;)).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your names below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = \"Arend van Dormalen and Oscar Ligthart\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "931b3dfcc3a02b92b499929fb27299cb",
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "EPS = float(np.finfo(np.float32).eps)\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e83ecfc2751cf2e6ff05d0c01d311673",
     "grade": false,
     "grade_id": "cell-fef7e20e54e6243b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1. Deep Q-Network (DQN) (10 (+ 2 bonus) points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e27fe8f72a248bbcf1f7a21e5550e657",
     "grade": true,
     "grade_id": "cell-39519f4ab05eb2a1",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.envs.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env is a TimeLimit wrapper around an env, so use env.env to look into the env (but otherwise you can forget about this)\n",
    "??env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "11a9c014ee5fbe790ce999428cc22658",
     "grade": false,
     "grade_id": "cell-2d83f70e62b99520",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Remember from the previous lab, that in order to optimize a policy we need to estimate the Q-values (e.g. estimate the *action* values). In the CartPole problem, our state is current position of the cart, the current velocity of the cart, the current (angular) position of the pole and the (angular) speed of the pole. As these are continuous variables, we have an infinite number of states (ignoring the fact that a digital computer can only represent finitely many states in finite memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9692b7acb09d018d9f80ce95685b81d5",
     "grade": false,
     "grade_id": "cell-bf2ac21267daffbb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Can you think of a way in which we can still use a tabular approach? Why would this work and can you think of an example problem where this would not work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3ffce6fca4071a1b543186db1b74cc98",
     "grade": true,
     "grade_id": "cell-b0fa2cb0c2cd2a63",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "#### Our Answer\n",
    "\n",
    "\n",
    "Arend: We can discretize the continous variables by creating bins. Since there are only two possible actions, the differences in policy are likely to be small when there is small variation in the input data. The pole and cart speed have a range from infinite to negative infinite. For these variables we would have to create two special bins, in which everything larger or smaller than a certain value will be included. \n",
    "\n",
    "We are not able to use this approach when small variations have a large inpact, or when the policy that has to be learned is more complex. For example, [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cd66b44d93f348df1e0ef8353377c879",
     "grade": false,
     "grade_id": "cell-0b3162496f5e6cf5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Implement Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84b9c38718c952ef8e62486fc9bf5e4a",
     "grade": false,
     "grade_id": "cell-96a86bcfa1ebc84a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will not use the tabular approach but approximate the Q-value function by a general approximator function. We will skip the linear case and directly use a two layer Neural Network. We use [PyTorch](https://pytorch.org/) to implement the network, as this will allow us to train it easily later. We can implement a model using `torch.nn.Sequential`, but with PyTorch it is actually very easy to implement the model (e.g. the forward pass) from scratch. Now implement the `QNetwork.forward` function that uses one hidden layer with ReLU activation (no output activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4ef7d14363dc2aa4beb638856c57a58c",
     "grade": false,
     "grade_id": "cell-216429a5dccf8a0e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2b9a48f9aee9ebc46da01c6f11cd789a",
     "grade": true,
     "grade_id": "cell-00ce108d640a5942",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1234)\n",
    "model = QNetwork(num_hidden)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(4, num_hidden), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(num_hidden, 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "# If you do not need backpropagation, wrap the computation in the torch.no_grad() context\n",
    "# This saves time and memory, and PyTorch complaints when converting to numpy\n",
    "with torch.no_grad():\n",
    "    assert np.allclose(model(x).numpy(), test_model(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7fc82889691dbd60ff9469b770744fcc",
     "grade": false,
     "grade_id": "cell-ca77eae2e62180cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.2 Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b3265bef151a12fe6969c378af76be2",
     "grade": false,
     "grade_id": "cell-b5b012e42dd2029e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What could be a problem with doing gradient updates on a sequence of state, action pairs $((s_t, a_t), (s_{t+1}, a_{t+1}) ...)$ observed while interacting with the environment? How will using *experience replay* help to overcome this (potential problem)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "75e1a8b00b2bfa9b7dd8805b371c6a4e",
     "grade": true,
     "grade_id": "cell-70a2e59541668a25",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "#### Our Answer\n",
    "\n",
    "[TO CHECK]\n",
    "Arend:\n",
    "\n",
    "One possible problem is that state-action pairs that are observed while interacting with the environment have strong temporal correlations. All samples are taken right after another. In many cases, the same action would be optimal.\n",
    "For example, a RL racecar going on a long straight section might learn to assign a very high value to going straight and will also attempt to go straight on curves. \n",
    "\n",
    "Experience replay prevents us from being fixated on a single region of the state-space, as the data from the experience replay will be sampled from all visited regions instead of the current region. By keeping the data diverse, it prevents reinforcing the same action.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9b3bbd8aaf3aade515736d0d07917a61",
     "grade": false,
     "grade_id": "cell-2c1d117a1a75fd69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now implement the `push` function that adds a transition to the replay buffer, and the sample function that returns a batch of samples. It should keep at most the maximum number of transitions. Also implement the `sample` function that samples a (random!) batch of data, for use during training (hint: you can use the function `random.sample`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "93a9f55f3950fe63b44aa84c5fd7f793",
     "grade": false,
     "grade_id": "cell-a3cc876e51eb157f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        \n",
    "        if len(self.memory) == self.capacity:\n",
    "            del self.memory[0]\n",
    "            \n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6865749b3a8810bdaaf1604a9cea42e7",
     "grade": true,
     "grade_id": "cell-3b90135921c4da76",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([-0.02467716,  0.03137446,  0.04458455,  0.01766512]), 0, 1.0, array([-0.02404967, -0.16435759,  0.04493785,  0.32407495]), False)]\n"
     ]
    }
   ],
   "source": [
    "capacity = 10\n",
    "memory = ReplayMemory(capacity)\n",
    "\n",
    "# Sample a transition\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_next, r, done, _ = env.step(a)\n",
    "\n",
    "# Push a transition\n",
    "memory.push((s, a, r, s_next, done))\n",
    "\n",
    "# Sample a batch size of 1\n",
    "print(memory.sample(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3c742d499c0f9b7f10d1c0c3a085236a",
     "grade": false,
     "grade_id": "cell-88f67e3c051da6a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.3 $\\epsilon$psilon greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "61d26d0dec0133f2aa737ed4711d6e08",
     "grade": false,
     "grade_id": "cell-aa3c7d1b3000f697",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to learn a good policy, we need to explore quite a bit initially. As we start to learn a good policy, we want to decrease the exploration. As the amount of exploration using an $\\epsilon$-greedy policy is controlled by $\\epsilon$, we can define an 'exploration scheme' by writing $\\epsilon$ as a function of time. There are many possible schemes, but we will use a simple one: we will start with only exploring (so taking random actions) at iteration 0, and then in 1000 iterations linearly anneal $\\epsilon$ such that after 1000 iterations we take random (exploration) actions with 5\\% probability (forever, as you never know if the environment will change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "270ab31d4bb29dc9a05223c16a4967a7",
     "grade": false,
     "grade_id": "cell-5789e7a792108576",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_epsilon(it):\n",
    "    \n",
    "    # random actions at first,\n",
    "    if it >= 1000:\n",
    "        epsilon = 0.05\n",
    "    else:\n",
    "        epsilon = 1 - 0.95 * it * (1/1000) \n",
    "    \n",
    "    # after 1000 iterations, e-greedy with epsilon being 0.5\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b1a81dd07e1b7a98d2cd06ebc171ebdd",
     "grade": true,
     "grade_id": "cell-40e66db45e742b2e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9e72a5be10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So what's an easy way to check?\n",
    "plt.plot([get_epsilon(it) for it in range(5000)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84685c23e4eb899d7fed3a87b7f8915e",
     "grade": false,
     "grade_id": "cell-a8b604c9998c6c3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now write a function that takes a state and uses the Q-network to select an ($\\epsilon$-greedy) action. It should return a random action with probability epsilon (which we will pass later). Note, you do not need to backpropagate through the model computations, so use `with torch.no_grad():` (see above for example). Unlike numpy, PyTorch has no argmax function, but Google is your friend... Note that to convert a PyTorch tensor with only 1 element (0 dimensional) to a simple python scalar (int or float), you can use the '.item()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "882f51819100c850120e73340aec387d",
     "grade": false,
     "grade_id": "cell-878ad3a637cfb51c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, state, epsilon):\n",
    "    \n",
    "    actions = model(torch.FloatTensor(state))\n",
    "    values, indices = actions.max(0)\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        a = np.random.choice(range(len(actions))).item()\n",
    "    else:\n",
    "        a = indices.item()\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21f939075cb0c8dde152dabf47568a9d",
     "grade": true,
     "grade_id": "cell-e895338d56bee477",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "a = select_action(model, s, 0.05)\n",
    "assert not torch.is_tensor(a)\n",
    "print (a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5d00ab2e5e0b39257771d0e778fda2d6",
     "grade": false,
     "grade_id": "cell-ec5e94e0b03f8aec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.4 Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4839aac72a80552046ebecc40c1615cf",
     "grade": false,
     "grade_id": "cell-d1a12cc97386fe56",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the function 'train' that samples a batch from the memory and performs a gradient step using some convenient PyTorch functionality. However, you still need to compute the Q-values for the (state, action) pairs in the experience, as well as their target (e.g. the value they should move towards). What is the target for a Q-learning update? What should be the target if `next_state` is terminal (e.g. `done`)?\n",
    "\n",
    "For computing the Q-values for the actions, note that the model returns all action values where you are only interested in a single action value. Because of the batch dimension, you can't use simple indexing, but you may want to have a look at [torch.gather](https://pytorch.org/docs/stable/torch.html?highlight=gather#torch.gather) or use [advanced indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html) (numpy tutorial but works mostly the same in PyTorch). Note, you should NOT modify the function train. You can view the size of a tensor `x` with `x.size()` (similar to `x.shape` in numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c466ee49add35cb1ec6a3e4a85f733c9",
     "grade": false,
     "grade_id": "cell-6c45485324b40081",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_q_val(model, state, action):\n",
    "\n",
    "    actions = model(state)\n",
    "\n",
    "    q_val = torch.gather(actions, 1, action.view(-1,1))\n",
    "\n",
    "    return q_val\n",
    "    \n",
    "    \n",
    "def compute_target(model, reward, next_state, done, discount_factor):\n",
    "    # done is a boolean (vector) that indicates if next_state is terminal (episode is done)\n",
    "    \n",
    "    actions = model(next_state)\n",
    "    _, indices = actions.max(1)\n",
    "\n",
    "    indices = torch.gather(actions, 1, indices.view(-1,1))\n",
    "    target = reward.view(indices.shape) + (discount_factor * indices)\n",
    "    # wat gebeurt er bij done?\n",
    "    \n",
    "    # set target to just the reward if next_state is terminal\n",
    "    target[done] = reward[done].view(target[done].shape)\n",
    "    return target\n",
    "    \n",
    "\n",
    "def train(model, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)\n",
    "    done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_val(model, state, action)\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_target(model, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "877c400001292b619e6871c1366524b9",
     "grade": true,
     "grade_id": "cell-b060b822eec4282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0c54261ded69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# We need a larger memory, fill with dummy data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtransition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c9c33f45ea3e>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl2018/lib/python3.6/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "# You may want to test your functions individually, but after you do so lets see if the method train works.\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "# Simple gradient descent may take long, so we will use Adam\n",
    "optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "\n",
    "# We need a larger memory, fill with dummy data\n",
    "transition = memory.sample(1)[0]\n",
    "memory = ReplayMemory(10 * batch_size)\n",
    "for i in range(batch_size):\n",
    "    memory.push(transition)\n",
    "\n",
    "# Now let's see if it works\n",
    "loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "\n",
    "print (loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2057dee580a43fb0442fe52557c0ac64",
     "grade": false,
     "grade_id": "cell-3eafd0ab49103f3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.5 Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "06dd71aae5c3c699f2b707b348a88107",
     "grade": false,
     "grade_id": "cell-36b8a04b393d8104",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now that you have implemented the training step, you should be able to put everything together. Implement the function `run_episodes` that runs a number of episodes of DQN training. It should return the durations (e.g. number of steps) of each episode. Note: we pass the train function as an argument such that we can swap it for a different training step later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c3f61b2ca270d84ab9b28d989dd65d4c",
     "grade": false,
     "grade_id": "cell-540a7d50ecc1d046",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        t = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Take actions until end of episode\n",
    "        for t in range(1000):\n",
    "            epsilon = get_epsilon(global_steps)\n",
    "            \n",
    "            action = select_action(model, state, epsilon)\n",
    "            \n",
    "            if i == num_episodes-1:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "                \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            memory.push((state, action, reward, next_state, done))\n",
    "            \n",
    "            # only sample if there is enough memory\n",
    "            if len(memory) > batch_size:\n",
    "                loss = train(model, memory, optimizer, batch_size, discount_factor)\n",
    "                \n",
    "            state = next_state  \n",
    "            global_steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        episode_durations.append(t)\n",
    "        \n",
    "    print(episode_durations)\n",
    "    env.close()\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ad583e34abcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mepisode_durations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducability\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "model = QNetwork(num_hidden)\n",
    "\n",
    "episode_durations = run_episodes(train, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "70d16eb61eae34605e8d7813a70a604a",
     "grade": true,
     "grade_id": "cell-928ecc11ed5c43d8",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episode_durations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-88f369d76629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcumsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_durations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode durations per episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'episode_durations' is not defined"
     ]
    }
   ],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1e106dba734da10d4d8b3bf90d6bb772",
     "grade": false,
     "grade_id": "cell-49e6bf74834a67ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1.6 Semi-gradient vs. true gradient (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "acf155c686f3916453a3d11d95994987",
     "grade": false,
     "grade_id": "cell-fc30be2a6983bc77",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Note that by using automatic differentiation in PyTorch, it is (relatively) easy to implement the true gradient method. Hint: PyTorch may complain about computing gradients for the target in [smooth_l1_loss](https://pytorch.org/docs/stable/nn.html?highlight=smooth_l1_loss#torch.nn.functional.smooth_l1_loss). How can you circumvent this problem? Implement the `train_true_gradient` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3d1e72257ed8c59175352e163f1bfdaf",
     "grade": true,
     "grade_id": "cell-71707640573b23d1",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_true_gradient(model, memory, optimizer, batch_size, discount_factor):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "model = QNetwork(num_hidden)\n",
    "\n",
    "episode_durations_true_gradient = run_episodes(\n",
    "    train_true_gradient, model, memory, env, num_episodes, batch_size, discount_factor, learn_rate)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.plot(smooth(episode_durations_true_gradient, 10))\n",
    "plt.title('Episode durations per episode')\n",
    "plt.legend(['Semi-gradient', 'True gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "95b462060bc00fccd7e8bc2ccc857215",
     "grade": false,
     "grade_id": "cell-b6fb5a1b0894fb4e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Which algorithm performs better? Is this what you would expect? Can you explain this?\n",
    "\n",
    "Note: you may want to play around with the number of episodes to answer this question, but please reset it to 100 before handing in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b2e5712195d20cce7d1a6afb34e24a41",
     "grade": true,
     "grade_id": "cell-d99dae457ea5bde6",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "de7203182e41f55f391af5892477e89d",
     "grade": false,
     "grade_id": "cell-6607b79e73a101a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Policy Gradient (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "951b88e9cd8396d088d3f80e6da9690c",
     "grade": false,
     "grade_id": "cell-083fe71da94aa7aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "So we have spent a lot of time working on *value based* methods. We will now switch to *policy based* methods, i.e. learn a policy directly rather than learn a value function from which the policy follows. Mention two advantages of using a policy based method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5c1f505cb22eca6eb3b8213ff23e60f",
     "grade": true,
     "grade_id": "cell-134510705650d5ac",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "OUR ANSWER\n",
    "\n",
    "1.\n",
    "\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "174629c02b62968e23fa6088c4d5763b",
     "grade": false,
     "grade_id": "cell-76a10fe31897025f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Policy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2bc16b45e6145226b8a6f5117003b7f5",
     "grade": false,
     "grade_id": "cell-34f0712f792bbcca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to do so, we will implement a Policy network. Although in general this does not have to be the case, we will use an architecture very similar to the Q-network (two layers with ReLU activation for the hidden layer). Since we have discrete actions, our model will output one value per action, where each value represents the (normalized!) log-probability of selecting that action. *Use the (log-)softmax activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "155baf230fd6deb5f6ccf93138fa3419",
     "grade": false,
     "grade_id": "cell-6a31440f9477f963",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        return F.log_softmax(self.l2(x),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3cb94e04b03fa4b663bcf38a96ef656d",
     "grade": true,
     "grade_id": "cell-9d280fe6520edc91",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4578, 0.5422],\n",
      "        [0.4657, 0.5343],\n",
      "        [0.4563, 0.5437],\n",
      "        [0.4634, 0.5366],\n",
      "        [0.4564, 0.5436],\n",
      "        [0.4725, 0.5275],\n",
      "        [0.4769, 0.5231],\n",
      "        [0.4834, 0.5166],\n",
      "        [0.4797, 0.5203],\n",
      "        [0.4618, 0.5382]], grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1234)\n",
    "model = PolicyNetwork(num_hidden)\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "log_p = model(x)\n",
    "\n",
    "# Does the outcome make sense?\n",
    "print(log_p.exp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "619c714e930c0d167304597d188f229b",
     "grade": false,
     "grade_id": "cell-35294ca4eda15b11",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Monte Carlo REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "93ed9cbcf70541f5a04709ee89a16e78",
     "grade": false,
     "grade_id": "cell-44f33e587542974d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the *Monte Carlo* policy gradient algorithm. Remember from lab 1 that this means that we will estimate returns for states by sample episodes. Compared to DQN, this means that we do *not* perform an update step at every environment step, but only at the end of each episode. This means that we should generate an episode of data, compute the REINFORCE loss (which requires computing the returns) and then perform a gradient step.\n",
    "\n",
    "To help you, we already implemented a few functions that you can (but do not have to) use.\n",
    "\n",
    "* You can use `torch.multinomial` to sample from a categorical distribution.\n",
    "* The REINFORCE loss is defined as $- \\sum_t \\log \\pi_\\theta(a_t|s_t) G_t$, which means that you should compute the (discounted) return $G_t$ for all $t$. Make sure that you do this in **linear time**, otherwise your algorithm will be very slow! Note the - (minus) since you want to maximize return while you want to minimize the loss.\n",
    "* Importantly, you should **normalize the returns** (not the rewards!, e.g. subtract mean and divide by standard deviation within the episode) before computing the loss, or your estimator will have very high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b2c75181678fed25fcc7c8b39bb7de3",
     "grade": true,
     "grade_id": "cell-3f6e32c4931392bf",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select_action(model, state):\n",
    "    # Samples an action according to the probability distribution induced by the model\n",
    "    # Also returns the log_probability\n",
    "    \n",
    "\n",
    "    log_p = model(torch.FloatTensor(state))\n",
    "    #log_p = model(torch.tensor(state, dtype=torch.float))\n",
    "    #action = np.random.choice(range(len(log_p)), p=log_p)\n",
    "    action = torch.multinomial(log_p.exp(),1)\n",
    "    \n",
    "    return action, torch.gather(log_p, 1, action)\n",
    "\n",
    "\n",
    "def run_episode(env, model):\n",
    "    \n",
    "    state = env.reset()    \n",
    "    episode = []\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        action, log_p = select_action(model, [state])\n",
    "        \n",
    "        action = action.item()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        episode.append((state, log_p, reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode\n",
    "\n",
    "def compute_reinforce_loss(episode, discount_factor):\n",
    "    # Compute the reinforce loss\n",
    "    # Make sure that your function runs in LINEAR TIME\n",
    "    # Don't forget to normalize your RETURNS (not rewards)\n",
    "    # Note that the rewards/returns should be maximized \n",
    "    # while the loss should be minimized so you need a - somewhere\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    G = 0\n",
    "    returns = []\n",
    "    \n",
    "    for state, log_p, reward in reversed(episode):\n",
    "        G = discount_factor * G + reward\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    # normalize returns\n",
    "    #mean_returns = np.mean(returns)\n",
    "    #mean_stddev = np.std(returns)    \n",
    "    #returns = (returns - mean_returns)/ (mean_stddev + 1e-8)\n",
    "    \n",
    "    returns = torch.FloatTensor(returns)\n",
    "    returns = (returns-returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        \n",
    "    loss = 0\n",
    "    for step, G in zip(episode, returns):\n",
    "        _, log_p, _ = step  \n",
    "        loss += -log_p * G\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def run_episodes_policy_gradient(model, env, num_episodes, discount_factor, learn_rate):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    episode_durations = []\n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        episode = run_episode(env, model)\n",
    "        loss = compute_reinforce_loss(episode, discount_factor)\n",
    "        \n",
    "        # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                  .format(i, len(episode), '\\033[92m' if len(episode) >= 195 else '\\033[99m'))\n",
    "        episode_durations.append(len(episode))\n",
    "        \n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 0 finished after 19 steps\n",
      "\u001b[99m Episode 10 finished after 17 steps\n",
      "\u001b[99m Episode 20 finished after 27 steps\n",
      "\u001b[99m Episode 30 finished after 34 steps\n",
      "\u001b[92m Episode 40 finished after 200 steps\n",
      "\u001b[99m Episode 50 finished after 71 steps\n",
      "\u001b[99m Episode 60 finished after 57 steps\n",
      "\u001b[99m Episode 70 finished after 33 steps\n",
      "\u001b[99m Episode 80 finished after 41 steps\n",
      "\u001b[99m Episode 90 finished after 74 steps\n",
      "\u001b[92m Episode 100 finished after 200 steps\n",
      "\u001b[99m Episode 110 finished after 121 steps\n",
      "\u001b[99m Episode 120 finished after 120 steps\n",
      "\u001b[99m Episode 130 finished after 58 steps\n",
      "\u001b[99m Episode 140 finished after 62 steps\n",
      "\u001b[99m Episode 150 finished after 102 steps\n",
      "\u001b[99m Episode 160 finished after 70 steps\n",
      "\u001b[92m Episode 170 finished after 200 steps\n",
      "\u001b[92m Episode 180 finished after 200 steps\n",
      "\u001b[99m Episode 190 finished after 87 steps\n",
      "\u001b[99m Episode 200 finished after 74 steps\n",
      "\u001b[99m Episode 210 finished after 106 steps\n",
      "\u001b[99m Episode 220 finished after 97 steps\n",
      "\u001b[92m Episode 230 finished after 200 steps\n",
      "\u001b[99m Episode 240 finished after 192 steps\n",
      "\u001b[92m Episode 250 finished after 200 steps\n",
      "\u001b[99m Episode 260 finished after 94 steps\n",
      "\u001b[99m Episode 270 finished after 150 steps\n",
      "\u001b[99m Episode 280 finished after 167 steps\n",
      "\u001b[99m Episode 290 finished after 143 steps\n",
      "\u001b[99m Episode 300 finished after 118 steps\n",
      "\u001b[99m Episode 310 finished after 136 steps\n",
      "\u001b[99m Episode 320 finished after 135 steps\n",
      "\u001b[99m Episode 330 finished after 145 steps\n",
      "\u001b[99m Episode 340 finished after 105 steps\n",
      "\u001b[99m Episode 350 finished after 89 steps\n",
      "\u001b[99m Episode 360 finished after 168 steps\n",
      "\u001b[92m Episode 370 finished after 200 steps\n",
      "\u001b[99m Episode 380 finished after 148 steps\n",
      "\u001b[99m Episode 390 finished after 106 steps\n",
      "\u001b[99m Episode 400 finished after 149 steps\n",
      "\u001b[92m Episode 410 finished after 200 steps\n",
      "\u001b[99m Episode 420 finished after 110 steps\n",
      "\u001b[99m Episode 430 finished after 156 steps\n",
      "\u001b[92m Episode 440 finished after 200 steps\n",
      "\u001b[99m Episode 450 finished after 132 steps\n",
      "\u001b[99m Episode 460 finished after 100 steps\n",
      "\u001b[99m Episode 470 finished after 67 steps\n",
      "\u001b[99m Episode 480 finished after 81 steps\n",
      "\u001b[99m Episode 490 finished after 105 steps\n",
      "\u001b[99m Episode 500 finished after 82 steps\n",
      "\u001b[99m Episode 510 finished after 65 steps\n",
      "\u001b[99m Episode 520 finished after 63 steps\n",
      "\u001b[99m Episode 530 finished after 85 steps\n",
      "\u001b[99m Episode 540 finished after 114 steps\n",
      "\u001b[99m Episode 550 finished after 151 steps\n",
      "\u001b[92m Episode 560 finished after 200 steps\n",
      "\u001b[92m Episode 570 finished after 200 steps\n",
      "\u001b[99m Episode 580 finished after 188 steps\n",
      "\u001b[99m Episode 590 finished after 168 steps\n",
      "\u001b[99m Episode 600 finished after 177 steps\n",
      "\u001b[99m Episode 610 finished after 155 steps\n",
      "\u001b[99m Episode 620 finished after 193 steps\n",
      "\u001b[99m Episode 630 finished after 164 steps\n",
      "\u001b[99m Episode 640 finished after 160 steps\n",
      "\u001b[99m Episode 650 finished after 170 steps\n",
      "\u001b[99m Episode 660 finished after 176 steps\n",
      "\u001b[99m Episode 670 finished after 188 steps\n",
      "\u001b[99m Episode 680 finished after 169 steps\n",
      "\u001b[99m Episode 690 finished after 132 steps\n",
      "\u001b[99m Episode 700 finished after 118 steps\n",
      "\u001b[99m Episode 710 finished after 118 steps\n",
      "\u001b[99m Episode 720 finished after 129 steps\n",
      "\u001b[99m Episode 730 finished after 149 steps\n",
      "\u001b[99m Episode 740 finished after 142 steps\n",
      "\u001b[99m Episode 750 finished after 128 steps\n",
      "\u001b[99m Episode 760 finished after 96 steps\n",
      "\u001b[99m Episode 770 finished after 36 steps\n",
      "\u001b[99m Episode 780 finished after 42 steps\n",
      "\u001b[99m Episode 790 finished after 50 steps\n",
      "\u001b[99m Episode 800 finished after 30 steps\n",
      "\u001b[99m Episode 810 finished after 64 steps\n",
      "\u001b[99m Episode 820 finished after 94 steps\n",
      "\u001b[99m Episode 830 finished after 98 steps\n",
      "\u001b[99m Episode 840 finished after 101 steps\n",
      "\u001b[99m Episode 850 finished after 90 steps\n",
      "\u001b[99m Episode 860 finished after 54 steps\n",
      "\u001b[99m Episode 870 finished after 60 steps\n",
      "\u001b[99m Episode 880 finished after 73 steps\n",
      "\u001b[99m Episode 890 finished after 87 steps\n",
      "\u001b[99m Episode 900 finished after 74 steps\n",
      "\u001b[99m Episode 910 finished after 91 steps\n",
      "\u001b[99m Episode 920 finished after 84 steps\n",
      "\u001b[99m Episode 930 finished after 94 steps\n",
      "\u001b[99m Episode 940 finished after 88 steps\n",
      "\u001b[99m Episode 950 finished after 92 steps\n",
      "\u001b[99m Episode 960 finished after 85 steps\n",
      "\u001b[99m Episode 970 finished after 84 steps\n",
      "\u001b[99m Episode 980 finished after 80 steps\n",
      "\u001b[99m Episode 990 finished after 84 steps\n",
      "\u001b[99m Episode 1000 finished after 65 steps\n",
      "\u001b[99m Episode 1010 finished after 68 steps\n",
      "\u001b[99m Episode 1020 finished after 79 steps\n",
      "\u001b[99m Episode 1030 finished after 101 steps\n",
      "\u001b[99m Episode 1040 finished after 99 steps\n",
      "\u001b[99m Episode 1050 finished after 91 steps\n",
      "\u001b[99m Episode 1060 finished after 85 steps\n",
      "\u001b[99m Episode 1070 finished after 91 steps\n",
      "\u001b[99m Episode 1080 finished after 79 steps\n",
      "\u001b[99m Episode 1090 finished after 73 steps\n",
      "\u001b[99m Episode 1100 finished after 88 steps\n",
      "\u001b[99m Episode 1110 finished after 119 steps\n",
      "\u001b[99m Episode 1120 finished after 126 steps\n",
      "\u001b[99m Episode 1130 finished after 96 steps\n",
      "\u001b[99m Episode 1140 finished after 103 steps\n",
      "\u001b[99m Episode 1150 finished after 124 steps\n",
      "\u001b[99m Episode 1160 finished after 126 steps\n",
      "\u001b[99m Episode 1170 finished after 110 steps\n",
      "\u001b[99m Episode 1180 finished after 81 steps\n",
      "\u001b[99m Episode 1190 finished after 73 steps\n",
      "\u001b[99m Episode 1200 finished after 69 steps\n",
      "\u001b[99m Episode 1210 finished after 64 steps\n",
      "\u001b[99m Episode 1220 finished after 78 steps\n",
      "\u001b[99m Episode 1230 finished after 87 steps\n",
      "\u001b[99m Episode 1240 finished after 86 steps\n",
      "\u001b[99m Episode 1250 finished after 84 steps\n",
      "\u001b[99m Episode 1260 finished after 104 steps\n",
      "\u001b[99m Episode 1270 finished after 115 steps\n",
      "\u001b[99m Episode 1280 finished after 134 steps\n",
      "\u001b[99m Episode 1290 finished after 119 steps\n",
      "\u001b[99m Episode 1300 finished after 147 steps\n",
      "\u001b[99m Episode 1310 finished after 167 steps\n",
      "\u001b[92m Episode 1320 finished after 200 steps\n",
      "\u001b[92m Episode 1330 finished after 200 steps\n",
      "\u001b[92m Episode 1340 finished after 200 steps\n",
      "\u001b[99m Episode 1350 finished after 190 steps\n",
      "\u001b[92m Episode 1360 finished after 196 steps\n",
      "\u001b[99m Episode 1370 finished after 133 steps\n",
      "\u001b[99m Episode 1380 finished after 109 steps\n",
      "\u001b[99m Episode 1390 finished after 101 steps\n",
      "\u001b[99m Episode 1400 finished after 109 steps\n",
      "\u001b[99m Episode 1410 finished after 86 steps\n",
      "\u001b[99m Episode 1420 finished after 89 steps\n",
      "\u001b[99m Episode 1430 finished after 105 steps\n",
      "\u001b[99m Episode 1440 finished after 111 steps\n",
      "\u001b[99m Episode 1450 finished after 97 steps\n",
      "\u001b[99m Episode 1460 finished after 131 steps\n",
      "\u001b[99m Episode 1470 finished after 176 steps\n",
      "\u001b[92m Episode 1480 finished after 200 steps\n",
      "\u001b[92m Episode 1490 finished after 200 steps\n",
      "\u001b[92m Episode 1500 finished after 200 steps\n",
      "\u001b[99m Episode 1510 finished after 182 steps\n",
      "\u001b[92m Episode 1520 finished after 195 steps\n",
      "\u001b[99m Episode 1530 finished after 179 steps\n",
      "\u001b[99m Episode 1540 finished after 139 steps\n",
      "\u001b[99m Episode 1550 finished after 89 steps\n",
      "\u001b[99m Episode 1560 finished after 84 steps\n",
      "\u001b[99m Episode 1570 finished after 65 steps\n",
      "\u001b[99m Episode 1580 finished after 94 steps\n",
      "\u001b[99m Episode 1590 finished after 82 steps\n",
      "\u001b[99m Episode 1600 finished after 149 steps\n",
      "\u001b[99m Episode 1610 finished after 143 steps\n",
      "\u001b[99m Episode 1620 finished after 111 steps\n",
      "\u001b[99m Episode 1630 finished after 147 steps\n",
      "\u001b[99m Episode 1640 finished after 153 steps\n",
      "\u001b[99m Episode 1650 finished after 116 steps\n",
      "\u001b[99m Episode 1660 finished after 151 steps\n",
      "\u001b[99m Episode 1670 finished after 138 steps\n",
      "\u001b[99m Episode 1680 finished after 155 steps\n",
      "\u001b[99m Episode 1690 finished after 105 steps\n",
      "\u001b[99m Episode 1700 finished after 156 steps\n",
      "\u001b[92m Episode 1710 finished after 200 steps\n",
      "\u001b[92m Episode 1720 finished after 200 steps\n",
      "\u001b[92m Episode 1730 finished after 200 steps\n",
      "\u001b[92m Episode 1740 finished after 200 steps\n",
      "\u001b[92m Episode 1750 finished after 200 steps\n",
      "\u001b[92m Episode 1760 finished after 200 steps\n",
      "\u001b[92m Episode 1770 finished after 200 steps\n",
      "\u001b[92m Episode 1780 finished after 200 steps\n",
      "\u001b[92m Episode 1790 finished after 200 steps\n",
      "\u001b[92m Episode 1800 finished after 200 steps\n",
      "\u001b[92m Episode 1810 finished after 200 steps\n",
      "\u001b[92m Episode 1820 finished after 195 steps\n",
      "\u001b[92m Episode 1830 finished after 200 steps\n",
      "\u001b[92m Episode 1840 finished after 200 steps\n",
      "\u001b[92m Episode 1850 finished after 200 steps\n",
      "\u001b[92m Episode 1860 finished after 200 steps\n",
      "\u001b[92m Episode 1870 finished after 200 steps\n",
      "\u001b[92m Episode 1880 finished after 200 steps\n",
      "\u001b[92m Episode 1890 finished after 200 steps\n",
      "\u001b[92m Episode 1900 finished after 200 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Episode 1910 finished after 200 steps\n",
      "\u001b[92m Episode 1920 finished after 200 steps\n",
      "\u001b[92m Episode 1930 finished after 200 steps\n",
      "\u001b[92m Episode 1940 finished after 200 steps\n",
      "\u001b[92m Episode 1950 finished after 200 steps\n",
      "\u001b[99m Episode 1960 finished after 42 steps\n",
      "\u001b[99m Episode 1970 finished after 39 steps\n",
      "\u001b[99m Episode 1980 finished after 12 steps\n",
      "\u001b[92m Episode 1990 finished after 200 steps\n",
      "\u001b[99m Episode 2000 finished after 12 steps\n",
      "\u001b[99m Episode 2010 finished after 162 steps\n",
      "\u001b[99m Episode 2020 finished after 166 steps\n",
      "\u001b[99m Episode 2030 finished after 129 steps\n",
      "\u001b[99m Episode 2040 finished after 80 steps\n",
      "\u001b[99m Episode 2050 finished after 74 steps\n",
      "\u001b[99m Episode 2060 finished after 36 steps\n",
      "\u001b[99m Episode 2070 finished after 16 steps\n",
      "\u001b[99m Episode 2080 finished after 63 steps\n",
      "\u001b[99m Episode 2090 finished after 96 steps\n",
      "\u001b[99m Episode 2100 finished after 114 steps\n",
      "\u001b[99m Episode 2110 finished after 136 steps\n",
      "\u001b[99m Episode 2120 finished after 165 steps\n",
      "\u001b[92m Episode 2130 finished after 200 steps\n",
      "\u001b[92m Episode 2140 finished after 200 steps\n",
      "\u001b[92m Episode 2150 finished after 200 steps\n",
      "\u001b[92m Episode 2160 finished after 198 steps\n",
      "\u001b[92m Episode 2170 finished after 197 steps\n",
      "\u001b[92m Episode 2180 finished after 195 steps\n",
      "\u001b[99m Episode 2190 finished after 175 steps\n",
      "\u001b[99m Episode 2200 finished after 189 steps\n",
      "\u001b[99m Episode 2210 finished after 162 steps\n",
      "\u001b[99m Episode 2220 finished after 185 steps\n",
      "\u001b[92m Episode 2230 finished after 200 steps\n",
      "\u001b[92m Episode 2240 finished after 200 steps\n",
      "\u001b[92m Episode 2250 finished after 200 steps\n",
      "\u001b[92m Episode 2260 finished after 200 steps\n",
      "\u001b[92m Episode 2270 finished after 200 steps\n",
      "\u001b[99m Episode 2280 finished after 44 steps\n",
      "\u001b[99m Episode 2290 finished after 12 steps\n",
      "\u001b[99m Episode 2300 finished after 151 steps\n",
      "\u001b[99m Episode 2310 finished after 155 steps\n",
      "\u001b[99m Episode 2320 finished after 132 steps\n",
      "\u001b[99m Episode 2330 finished after 140 steps\n",
      "\u001b[99m Episode 2340 finished after 25 steps\n",
      "\u001b[99m Episode 2350 finished after 132 steps\n",
      "\u001b[99m Episode 2360 finished after 153 steps\n",
      "\u001b[92m Episode 2370 finished after 199 steps\n",
      "\u001b[92m Episode 2380 finished after 200 steps\n",
      "\u001b[99m Episode 2390 finished after 186 steps\n",
      "\u001b[92m Episode 2400 finished after 200 steps\n",
      "\u001b[92m Episode 2410 finished after 200 steps\n",
      "\u001b[92m Episode 2420 finished after 200 steps\n",
      "\u001b[99m Episode 2430 finished after 176 steps\n",
      "\u001b[92m Episode 2440 finished after 200 steps\n",
      "\u001b[92m Episode 2450 finished after 200 steps\n",
      "\u001b[92m Episode 2460 finished after 200 steps\n",
      "\u001b[99m Episode 2470 finished after 74 steps\n",
      "\u001b[92m Episode 2480 finished after 196 steps\n",
      "\u001b[92m Episode 2490 finished after 200 steps\n",
      "\u001b[92m Episode 2500 finished after 200 steps\n",
      "\u001b[92m Episode 2510 finished after 200 steps\n",
      "\u001b[92m Episode 2520 finished after 200 steps\n",
      "\u001b[92m Episode 2530 finished after 200 steps\n",
      "\u001b[92m Episode 2540 finished after 200 steps\n",
      "\u001b[92m Episode 2550 finished after 200 steps\n",
      "\u001b[92m Episode 2560 finished after 200 steps\n",
      "\u001b[92m Episode 2570 finished after 200 steps\n",
      "\u001b[92m Episode 2580 finished after 200 steps\n",
      "\u001b[92m Episode 2590 finished after 200 steps\n",
      "\u001b[92m Episode 2600 finished after 200 steps\n",
      "\u001b[92m Episode 2610 finished after 200 steps\n",
      "\u001b[92m Episode 2620 finished after 200 steps\n",
      "\u001b[92m Episode 2630 finished after 200 steps\n",
      "\u001b[92m Episode 2640 finished after 200 steps\n",
      "\u001b[92m Episode 2650 finished after 200 steps\n",
      "\u001b[92m Episode 2660 finished after 200 steps\n",
      "\u001b[92m Episode 2670 finished after 200 steps\n",
      "\u001b[92m Episode 2680 finished after 200 steps\n",
      "\u001b[92m Episode 2690 finished after 200 steps\n",
      "\u001b[92m Episode 2700 finished after 200 steps\n",
      "\u001b[92m Episode 2710 finished after 200 steps\n",
      "\u001b[92m Episode 2720 finished after 200 steps\n",
      "\u001b[92m Episode 2730 finished after 200 steps\n",
      "\u001b[92m Episode 2740 finished after 200 steps\n",
      "\u001b[92m Episode 2750 finished after 200 steps\n",
      "\u001b[92m Episode 2760 finished after 200 steps\n",
      "\u001b[92m Episode 2770 finished after 200 steps\n",
      "\u001b[92m Episode 2780 finished after 200 steps\n",
      "\u001b[92m Episode 2790 finished after 200 steps\n",
      "\u001b[92m Episode 2800 finished after 200 steps\n",
      "\u001b[92m Episode 2810 finished after 200 steps\n",
      "\u001b[92m Episode 2820 finished after 200 steps\n",
      "\u001b[92m Episode 2830 finished after 200 steps\n",
      "\u001b[92m Episode 2840 finished after 200 steps\n",
      "\u001b[92m Episode 2850 finished after 200 steps\n",
      "\u001b[92m Episode 2860 finished after 200 steps\n",
      "\u001b[92m Episode 2870 finished after 200 steps\n",
      "\u001b[92m Episode 2880 finished after 200 steps\n",
      "\u001b[92m Episode 2890 finished after 200 steps\n",
      "\u001b[92m Episode 2900 finished after 200 steps\n",
      "\u001b[92m Episode 2910 finished after 200 steps\n",
      "\u001b[92m Episode 2920 finished after 200 steps\n",
      "\u001b[99m Episode 2930 finished after 98 steps\n",
      "\u001b[99m Episode 2940 finished after 53 steps\n",
      "\u001b[92m Episode 2950 finished after 200 steps\n",
      "\u001b[92m Episode 2960 finished after 200 steps\n",
      "\u001b[92m Episode 2970 finished after 200 steps\n",
      "\u001b[92m Episode 2980 finished after 200 steps\n",
      "\u001b[92m Episode 2990 finished after 200 steps\n",
      "\u001b[92m Episode 3000 finished after 200 steps\n",
      "\u001b[92m Episode 3010 finished after 200 steps\n",
      "\u001b[92m Episode 3020 finished after 200 steps\n",
      "\u001b[92m Episode 3030 finished after 200 steps\n",
      "\u001b[92m Episode 3040 finished after 200 steps\n",
      "\u001b[92m Episode 3050 finished after 200 steps\n",
      "\u001b[92m Episode 3060 finished after 200 steps\n",
      "\u001b[92m Episode 3070 finished after 200 steps\n",
      "\u001b[92m Episode 3080 finished after 200 steps\n",
      "\u001b[92m Episode 3090 finished after 200 steps\n",
      "\u001b[92m Episode 3100 finished after 200 steps\n",
      "\u001b[92m Episode 3110 finished after 200 steps\n",
      "\u001b[92m Episode 3120 finished after 200 steps\n",
      "\u001b[92m Episode 3130 finished after 200 steps\n",
      "\u001b[92m Episode 3140 finished after 200 steps\n",
      "\u001b[92m Episode 3150 finished after 200 steps\n",
      "\u001b[99m Episode 3160 finished after 11 steps\n",
      "\u001b[99m Episode 3170 finished after 181 steps\n",
      "\u001b[99m Episode 3180 finished after 15 steps\n",
      "\u001b[99m Episode 3190 finished after 15 steps\n",
      "\u001b[99m Episode 3200 finished after 17 steps\n",
      "\u001b[99m Episode 3210 finished after 12 steps\n",
      "\u001b[99m Episode 3220 finished after 156 steps\n",
      "\u001b[99m Episode 3230 finished after 182 steps\n",
      "\u001b[92m Episode 3240 finished after 200 steps\n",
      "\u001b[92m Episode 3250 finished after 200 steps\n",
      "\u001b[92m Episode 3260 finished after 200 steps\n",
      "\u001b[92m Episode 3270 finished after 200 steps\n",
      "\u001b[92m Episode 3280 finished after 200 steps\n",
      "\u001b[92m Episode 3290 finished after 200 steps\n",
      "\u001b[92m Episode 3300 finished after 200 steps\n",
      "\u001b[92m Episode 3310 finished after 200 steps\n",
      "\u001b[92m Episode 3320 finished after 200 steps\n",
      "\u001b[92m Episode 3330 finished after 200 steps\n",
      "\u001b[92m Episode 3340 finished after 200 steps\n",
      "\u001b[92m Episode 3350 finished after 200 steps\n",
      "\u001b[92m Episode 3360 finished after 200 steps\n",
      "\u001b[92m Episode 3370 finished after 200 steps\n",
      "\u001b[92m Episode 3380 finished after 200 steps\n",
      "\u001b[92m Episode 3390 finished after 200 steps\n",
      "\u001b[92m Episode 3400 finished after 200 steps\n",
      "\u001b[92m Episode 3410 finished after 200 steps\n",
      "\u001b[92m Episode 3420 finished after 200 steps\n",
      "\u001b[92m Episode 3430 finished after 200 steps\n",
      "\u001b[92m Episode 3440 finished after 200 steps\n",
      "\u001b[92m Episode 3450 finished after 200 steps\n",
      "\u001b[92m Episode 3460 finished after 200 steps\n",
      "\u001b[92m Episode 3470 finished after 200 steps\n",
      "\u001b[92m Episode 3480 finished after 200 steps\n",
      "\u001b[92m Episode 3490 finished after 200 steps\n",
      "\u001b[92m Episode 3500 finished after 200 steps\n",
      "\u001b[92m Episode 3510 finished after 200 steps\n",
      "\u001b[92m Episode 3520 finished after 200 steps\n",
      "\u001b[92m Episode 3530 finished after 200 steps\n",
      "\u001b[92m Episode 3540 finished after 200 steps\n",
      "\u001b[92m Episode 3550 finished after 200 steps\n",
      "\u001b[92m Episode 3560 finished after 200 steps\n",
      "\u001b[92m Episode 3570 finished after 200 steps\n",
      "\u001b[92m Episode 3580 finished after 200 steps\n",
      "\u001b[92m Episode 3590 finished after 200 steps\n",
      "\u001b[92m Episode 3600 finished after 200 steps\n",
      "\u001b[92m Episode 3610 finished after 200 steps\n",
      "\u001b[92m Episode 3620 finished after 200 steps\n",
      "\u001b[92m Episode 3630 finished after 200 steps\n",
      "\u001b[92m Episode 3640 finished after 200 steps\n",
      "\u001b[92m Episode 3650 finished after 200 steps\n",
      "\u001b[92m Episode 3660 finished after 200 steps\n",
      "\u001b[92m Episode 3670 finished after 200 steps\n",
      "\u001b[92m Episode 3680 finished after 200 steps\n",
      "\u001b[92m Episode 3690 finished after 200 steps\n",
      "\u001b[92m Episode 3700 finished after 200 steps\n",
      "\u001b[92m Episode 3710 finished after 200 steps\n",
      "\u001b[92m Episode 3720 finished after 200 steps\n",
      "\u001b[92m Episode 3730 finished after 200 steps\n",
      "\u001b[92m Episode 3740 finished after 200 steps\n",
      "\u001b[92m Episode 3750 finished after 200 steps\n",
      "\u001b[92m Episode 3760 finished after 200 steps\n",
      "\u001b[92m Episode 3770 finished after 200 steps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Episode 3780 finished after 200 steps\n",
      "\u001b[92m Episode 3790 finished after 200 steps\n",
      "\u001b[92m Episode 3800 finished after 200 steps\n",
      "\u001b[92m Episode 3810 finished after 200 steps\n",
      "\u001b[92m Episode 3820 finished after 200 steps\n",
      "\u001b[92m Episode 3830 finished after 200 steps\n",
      "\u001b[92m Episode 3840 finished after 200 steps\n",
      "\u001b[92m Episode 3850 finished after 200 steps\n",
      "\u001b[92m Episode 3860 finished after 200 steps\n",
      "\u001b[92m Episode 3870 finished after 200 steps\n",
      "\u001b[92m Episode 3880 finished after 200 steps\n",
      "\u001b[92m Episode 3890 finished after 200 steps\n",
      "\u001b[92m Episode 3900 finished after 200 steps\n",
      "\u001b[92m Episode 3910 finished after 200 steps\n",
      "\u001b[92m Episode 3920 finished after 200 steps\n",
      "\u001b[92m Episode 3930 finished after 200 steps\n",
      "\u001b[92m Episode 3940 finished after 200 steps\n",
      "\u001b[92m Episode 3950 finished after 200 steps\n",
      "\u001b[92m Episode 3960 finished after 200 steps\n",
      "\u001b[92m Episode 3970 finished after 200 steps\n",
      "\u001b[92m Episode 3980 finished after 200 steps\n",
      "\u001b[92m Episode 3990 finished after 200 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6c795acd30>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feel free to play around with the parameters!\n",
    "num_episodes = 4000\n",
    "discount_factor = 0.99\n",
    "learn_rate = 0.01\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "model = PolicyNetwork(num_hidden)\n",
    "\n",
    "episode_durations_policy_gradient = run_episodes_policy_gradient(\n",
    "    model, env, num_episodes, discount_factor, learn_rate)\n",
    "\n",
    "plt.plot(smooth(episode_durations_policy_gradient, 10))\n",
    "plt.title('Episode durations per episode')\n",
    "plt.legend(['Policy gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b9fe846472bc09094ba671593c4b40b4",
     "grade": false,
     "grade_id": "cell-af9c49b396393dc0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Actor-Critic (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ff32c0931b08aa9a5719639105a7b3e5",
     "grade": false,
     "grade_id": "cell-7eabad968ce02adf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will now implement the basic Actor-Critic algorithm, which means that instead of using Monte Carlo returns, we will bootstrap (1-step) returns using a critic (state-value function), so $G_t = R_t + \\gamma V(s_{t+1})$. What happens at the end of the episode? Hint: you may find it useful to have a look at the `train` method for DQN.\n",
    "\n",
    "* Note that we now have to train an actor (policy) and a critic (value network).\n",
    "* We will do this using a single optimizer, which means that we have to sum the loss for the actor and the critic into a single loss term. \n",
    "* For the critic, use the `smooth_l1_loss` like with DQN.\n",
    "* For the actor, the loss should be the REINFORCE loss, but with two differences:\n",
    "    - Instead of the Monte Carlo return $G_t$, use the one step return $G_{t:t+1}$ where the critic is used to bootstrap the value of $s_{t+1}$.\n",
    "    - Instead of normalizing the returns (which can be viewed as using the average as baseline and then scaling), we will use the estimated value $V(s_t)$ as baseline.\n",
    "* **Important**: note that you cannot use `with torch.no_grad():` to compute the critic value (for the current state) since you need gradients to train the critic! However, when using the value to compute the actor loss, you do not want to get gradients of the critic parameters w.r.t. the actor loss (e.g. your target and baseline must be constant)! Therefore, use `v.detach()` on the output of the critic when it is used in the loss term for the actor, this will make sure the value(s) are treated as a constant and no gradients will be backpropagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b649f137296d2c6e9ac367781f1b04e",
     "grade": true,
     "grade_id": "cell-5a7326fd2ab9349c",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "def select_action(model, state):\n",
    "    # Samples an action according to the probability distribution induced by the model\n",
    "    # Also returns the log_probability\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # action and log_p should be a 1 dimensional vector\n",
    "    n = len(state)\n",
    "    assert action.size() == (n, )\n",
    "    assert log_p.size() == (n, )\n",
    "    return action, log_p\n",
    "\n",
    "def train_actor_critic(actor, critic, optimizer, log_ps, state, reward, next_state, done, discount_factor):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # The loss is composed of the value_loss (for the critic) and the actor_loss\n",
    "    loss = value_loss + actor_loss\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), value_loss.item(), actor_loss.item()  # Returns a Python scalar, and releases history (similar to .detach())\n",
    "\n",
    "def run_episodes_actor_critic(actor, critic, envs, max_episodes, max_steps, discount_factor, actor_learn_rate, critic_learn_rate):\n",
    "    \n",
    "    # We can use a single optimizer for both the actor and the critic, even with separate learn rates\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': actor.parameters(), 'lr': actor_learn_rate},\n",
    "        {'params': critic.parameters(), 'lr': critic_learn_rate}\n",
    "    ])\n",
    "    \n",
    "    episode_durations = []\n",
    "    state = torch.tensor([env.reset() for env in envs], dtype=torch.float)\n",
    "    current_episode_lengths = torch.zeros(len(envs), dtype=torch.int64)\n",
    "    step_losses = []  # Keep track of losses for plotting\n",
    "    for i in range(max_steps):\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Step {i}, finished {len(episode_durations)} / {num_episodes} episodes, average episode duration of last 100 episodes: {np.mean(episode_durations[-100:])}\")\n",
    "        \n",
    "        action, log_ps = select_action(actor, state)\n",
    "        next_state, reward, done, _ = zip(*[env.step(a.item()) for env, a in zip(envs, action)])\n",
    "        \n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        done = torch.tensor(done, dtype=torch.uint8)  # Boolean\n",
    "        current_episode_lengths += 1\n",
    "        \n",
    "        losses = train_actor_critic(actor, critic, optimizer, log_ps, state, reward, next_state, done, discount_factor)\n",
    "        \n",
    "        step_losses.append(losses)\n",
    "        \n",
    "        # Reset envs that are done\n",
    "        next_state = torch.tensor([\n",
    "            env.reset() if d else s.tolist()\n",
    "            for env, s, d in zip(envs, next_state, done)\n",
    "        ], dtype=torch.float)\n",
    "        \n",
    "        episode_durations.extend(current_episode_lengths[done])\n",
    "        current_episode_lengths[done] = 0  # PyTorch can also work in place\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # Check if we have finished sufficiently many episodes\n",
    "        if len(episode_durations) >= max_episodes:\n",
    "            break\n",
    "        \n",
    "    return episode_durations[:max_episodes], step_losses  # In case we want exactly num_episodes returned\n",
    "\n",
    "\n",
    "num_envs = 16\n",
    "max_steps = 10000\n",
    "max_episodes = 10000\n",
    "discount_factor = 0.8\n",
    "lr_actor = 1e-3\n",
    "lr_critic = 1e-3\n",
    "seed = 42\n",
    "\n",
    "actor = PolicyNetwork(num_hidden)\n",
    "critic = ValueNetwork(num_hidden)\n",
    "\n",
    "envs = [gym.envs.make(\"CartPole-v0\") for i in range(num_envs)]\n",
    "\n",
    "for i, env in enumerate(envs):\n",
    "    env.seed(seed + i)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "episode_durations, step_losses = run_episodes_actor_critic(actor, critic, envs, max_episodes, max_steps, discount_factor, lr_actor, lr_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(smooth(episode_durations, 100))\n",
    "plt.title('Episode durations')\n",
    "plt.show()\n",
    "loss, v_loss, a_loss = zip(*step_losses)\n",
    "\n",
    "plt.plot(smooth(v_loss, 100))\n",
    "plt.title('Value loss')\n",
    "plt.show()\n",
    "plt.plot(smooth(a_loss, 100))\n",
    "plt.title('Actor loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "de8c4cba2ebd1a8bba2236f92a0b550c",
     "grade": false,
     "grade_id": "cell-8d15d4c9c0310bec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What is the difficulty of training AC algorithms? What could you try to do to overcome these difficulties? Hint: look at some online implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1e51e82a7730101dfd07b2f0e470d1b4",
     "grade": true,
     "grade_id": "cell-f68c6134a9df40b9",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5947c1e643f533003715ae8da659af9e",
     "grade": false,
     "grade_id": "cell-ad1138b69e6728a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Deep Reinforcement Learning (5 bonus points)\n",
    "Note that so far we used the state variables as input. However, the true power of Deep Learning is that we can directly learn from raw inputs, e.g. we can learn to balance the cart pole *by just looking at the screen*. This probably means that you need a deep(er) (convolutional) network, as well as tweaking some parameters, running for more iterations (perhaps on GPU) and do other tricks to stabilize learning. Can you get this to work? This will earn you bonus points!\n",
    "\n",
    "Hints:\n",
    "* You may want to use [Google Colab](https://colab.research.google.com/) such that you can benefit from GPU acceleration.\n",
    "* Even if you don't use Colab, save the weights of your final model and load it in the code here (see example below). Hand in the model file with the .ipynb in a .zip. We likely won't be able to run your training code during grading!\n",
    "* To run the code below, you need to install `torchvision`, for this uncomment the two lines in the cell below or run the command in a terminal. Note: you may need to restart the terminal after installing.\n",
    "* Preprocessing is already done for you, and the observation is the difference between two consequtive frames such that the model can 'see' (angular) speed from a single image. Now do you see why we (sometimes) use the word observation (and not state)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# conda install torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f660e1484fe2bf60d66467326eacb1ba",
     "grade": false,
     "grade_id": "cell-9c9dfa80827c5680",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "class CartPoleRawEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._env = gym.make('CartPole-v0', *args, **kwargs)  #.unwrapped\n",
    "        self.action_space = self._env.action_space\n",
    "        screen_height, screen_width = 40, 80  # TODO\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, \n",
    "            shape=(screen_height, screen_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        return self._env.seed(seed)\n",
    "    \n",
    "    def reset(self):\n",
    "        s = self._env.reset()\n",
    "        self.prev_screen = self.screen = self.get_screen()\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        s, r, done, info = self._env.step(action)\n",
    "        self.prev_screen = self.screen\n",
    "        self.screen = self.get_screen()\n",
    "        return self._get_observation(), r, done, info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        return self.screen - self.prev_screen\n",
    "    \n",
    "    def _get_cart_location(self, screen_width):\n",
    "        _env = self._env.unwrapped\n",
    "        world_width = _env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(_env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "    def get_screen(self):\n",
    "        screen = self._env.unwrapped.render(mode='rgb_array').transpose(\n",
    "            (2, 0, 1))  # transpose into torch order (CHW)\n",
    "        # Strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        screen = screen[:, screen_height * 4 // 10:screen_height * 8 // 10]\n",
    "        view_width = screen_height * 8 // 10\n",
    "        cart_location = self._get_cart_location(screen_width)\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescare, convert to torch tensor\n",
    "        # (this doesn't require a copy)\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        #return screen.unsqueeze(0).to(device)\n",
    "        return resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def close(self):\n",
    "        return self._env.close()\n",
    "\n",
    "raw_env = CartPoleRawEnv()\n",
    "s = raw_env.reset()\n",
    "\n",
    "# \n",
    "s, r, done, _ = raw_env.step(env.action_space.sample())\n",
    "\n",
    "raw_env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(raw_env.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n",
    "\n",
    "# Observations are (-1, 1) while we need to plot (0, 1) so show (rgb + 1) / 2\n",
    "plt.figure()\n",
    "plt.imshow((s.cpu().squeeze(0).permute(1, 2, 0).numpy() + 1) / 2,\n",
    "           interpolation='none')\n",
    "plt.title('Example observation')\n",
    "plt.show()\n",
    "raw_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe you should make it a bit deeper?\n",
    "class DeepPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(40 * 80 * 3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten\n",
    "        return F.log_softmax(self.l1(x.view(x.size(0), -1)), -1)\n",
    "    \n",
    "policy = DeepPolicy()\n",
    "filename = 'weights.pt'\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    print(f\"Loading weights from {filename}\")\n",
    "    weights = torch.load(filename)\n",
    "    \n",
    "    policy.load_state_dict(weights['policy'])\n",
    "    \n",
    "else:\n",
    "    # Train\n",
    "    \n",
    "    ### TODO some training here, maybe? Or run this on a different machine?\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    print(f\"Saving weights to {filename}\")\n",
    "    torch.save({\n",
    "        # You can add more here if you need, e.g. critic\n",
    "        'policy': policy.state_dict()  # Always save weights rather than objects\n",
    "    },\n",
    "    filename)\n",
    "    \n",
    "def bonus_get_action(x):\n",
    "    return policy(x).exp().multinomial(1)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b800bfb91f987f14e0c35bc0c41d538b",
     "grade": true,
     "grade_id": "cell-0d7bd58a23fdfabb",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "episode_durations = []\n",
    "for i in range(20):  # Not too many since it may take forever to render\n",
    "    test_env = CartPoleRawEnv()\n",
    "    test_env.seed(seed + i)\n",
    "    state = test_env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        with torch.no_grad():\n",
    "            action = bonus_get_action(state).item()\n",
    "        state, reward, done, _ = test_env.step(action)\n",
    "    episode_durations.append(steps)\n",
    "    test_env.close()\n",
    "    \n",
    "plt.plot(smooth(episode_durations, 100))\n",
    "plt.title('Episode durations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
